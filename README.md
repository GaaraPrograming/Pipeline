# ğŸš€ Proyecto: Pipeline ETL con Apache Airflow

**Autor:** Jeyner Suarez  
**Fecha:** Julio 2025  
**Email:** jeynersyarez@gmail.com  

---

## ğŸ¯ Objetivo

DiseÃ±ar e implementar un pipeline de procesamiento de datos desde un archivo CSV utilizando Apache Airflow. El pipeline debe automatizar las siguientes tareas:

- Ingesta de datos.
- Limpieza y normalizaciÃ³n.
- Transformaciones y agregaciones.
- Carga en base de datos (PostgreSQL).
- OrquestaciÃ³n con Airflow.
- Monitoreo bÃ¡sico.

---

## âš™ï¸ TecnologÃ­as Utilizadas

- Docker y Docker Compose  
- Apache Airflow 2.8.1  
- Python 3.9  
- PostgreSQL 13  
- Pandas

---

## ğŸ“ Estructura del Proyecto## ğŸ“ Estructura del Proyecto

- data_pipeline/
- â”‚
- â”œâ”€â”€ dags/
- â”‚ â””â”€â”€ dag_pipeline.py
- â”œâ”€â”€ data/
- â”‚ â”œâ”€â”€ raw_data.csv
- â”‚ â””â”€â”€ cleaned_data.csv # generado automÃ¡ticamente
- â”œâ”€â”€ scripts/
- â”œâ”€â”€ Dockerfile
- â”œâ”€â”€ docker-compose.yml
- â””â”€â”€ README.md

---

## ğŸ”„ Flujo del Pipeline

El DAG `pipeline_datos_csv` estÃ¡ compuesto por las siguientes tareas:

1. **Ingesta de Datos**  
   Se carga el archivo `raw_data.csv` desde la carpeta `/opt/airflow/data`.

2. **Limpieza y NormalizaciÃ³n**  
   - Se eliminan duplicados.
   - Se eliminan valores nulos.
   - Se convierten los campos de fecha al formato `datetime`.

3. **AgregaciÃ³n de Datos**  
   - Se calcula el monto total por cliente (`SUM(monto)`).
   - El resultado se guarda en `aggregated_data.csv`.

4. **Carga a PostgreSQL**  
   - Se crea la tabla `ventas_agrupadas` (si no existe).
   - Se insertan los datos agregados por cliente.

---

## ğŸ“Š Datos de Ejemplo

**Contenido de `raw_data.csv`:**

| id | fecha       | cliente   | producto   | monto | moneda |
|----|-------------|-----------|------------|-------|--------|
| 1  | 2024-06-01  | Cliente A | Producto X | 1200  | USD    |
| 2  | 2024-06-01  | Cliente B | Producto Y | 500   | COP    |
| 3  | 2024-06-02  | Cliente C | Producto x | 700   | USD    |
| 4  | 2024-06-02  | Cliente A | Producto z | 900   | PEN    |


---

## ğŸ“ˆ MÃ©tricas del Pipeline

- âœ… **Duplicados eliminados**: 0  
- âœ… **Registros eliminados por nulos**: 0  
- âœ… **Registros finales despuÃ©s de limpieza**: 4  
- âœ… **Clientes Ãºnicos en agregaciÃ³n**: 3  
- âœ… **Filas cargadas a PostgreSQL**: 3

---

## ğŸ”’ Sugerencias de Mejora

- Implementar logs persistentes y mÃ©tricas con Prometheus o Grafana.
- Agregar validaciones de esquema con `great_expectations` o `pydantic`.
- AÃ±adir manejo de errores con `try-except` en tareas crÃ­ticas.
- Usar particiones por fecha en la tabla de destino.
- AÃ±adir soporte para lectura de archivos JSON o Parquet.
- Integrar notificaciones por email o Slack para fallos.

---

## ğŸ§ª CÃ³mo Ejecutar el Proyecto

```bash
# 1. Apagar contenedores previos si estÃ¡n activos
docker-compose down --volumes

# 2. Levantar entorno y construir contenedores
docker-compose up --build -d

# 3. Inicializar base de datos de Airflow
docker-compose run --rm airflow airflow db init

# 4. Crear usuario administrador
docker-compose run --rm airflow airflow users create \
  --username admin --firstname Admin --lastname User \
  --role Admin --email admin@example.com --password admin

# 5. Acceder a la interfaz en http://localhost:8080


